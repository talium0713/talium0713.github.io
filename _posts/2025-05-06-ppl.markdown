---
layout: post
title:  "Policy-labeled Preference Learning: Is Preference Enough for RLHF?"
date:   2025-05-06 20:21:59 +00:00
image: images/PPL_img.png
categories: ['International Conference']
author: "Taehyun Cho"
authors: "<b>Taehyun Cho</b>, Seokhun Ju, Seungyub Han, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee"
venue: ICML 2025 <span style="color:red; font-weight:bold;">spotlight (Top 2.6%)</span>
paper: https://arxiv.org/pdf/2407.21260
arxiv: https://arxiv.org/abs/2407.21260
slides: 
code: 
---
Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward, 
we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information. 
We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making.
