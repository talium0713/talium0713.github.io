<!DOCTYPE HTML>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Taehyun Cho</title>
  <meta charset="utf-8">
  <meta name="author" content="Taehyun Cho">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/style.css">
  <link rel="canonical" href="https://talium0713.github.io/">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet">
</head>

<body style>
  <table style="width:100%;max-width:800px;margin:auto;border-collapse:separate;border-spacing:0;">
    <tr><td>
<!-- Header -->
<table style="width:100%;margin:auto;">
<tr>
<td style="padding:2.5%;width:60%;vertical-align:middle">
<h1>Taehyun Cho</h1>
<p>
I am a Ph.D. in Electrical and Computer Engineering from the
<a href="https://cml.snu.ac.kr//">Cognitive Machine Learning Laboratory</a> at
<a href="https://ece.snu.ac.kr/">Seoul National University</a>, advised by
<a href="https://cml.snu.ac.kr/?page_id=5948">Jungwoo Lee</a>.
My research focuses on reinforcement learning, human-aligned sequential decision-making, and uncertainty-aware learning.
I received my B.S. in Mathematics from <a href="https://math.korea.ac.kr">Korea University</a>.</p>
<p style="text-align:center">
E-mail: <a href="mailto:talium@cml.snu.ac.kr">talium@cml.snu.ac.kr</a>
</p>
<p style="text-align:center">
<a href="https://github.com/talium0713">GitHub</a>  / 
<a href="https://scholar.google.com/citations?user=kVi85ZgAAAAJ">Google Scholar</a>  / 
<a href="https://www.linkedin.com/in/taehyun-cho-510b752a0/">LinkedIn</a>  / 
<a href="/pdfs/cv_cth.pdf">CV</a>
</p>
</td>
<td style="padding:2.5%;width:40%;text-align:center">
<img style="width:70%;border-radius:50%;" src="images/img_cth.jpg" alt="profile photo">
</td>
</tr>
</table>
<!-- Research Interest -->
<table style="width:100%;margin:auto;">
<tr>
<td style="padding:2.5%;">
<h2>Research Interest</h2>
<p>
My academic research focuses on <b>sequential decision-making under uncertainty</b>, particularly in the context of <b>human feedback</b>.
I have extensively studied <b>distributional reinforcement learning (distRL)</b>, <b>reinforcement learning from human feedback (RLHF)</b>, and <b>regret analysis</b>,
aiming to bridge theory and practice.
</p>
<p>
Drawing inspiration from how humans make decisions, I aim to develop mathematical models and optimize for human-in-the-loop systems,
uncovering both theoretical insights and practical algorithms for robust decision-making.
Currently, I’m interested in <b>reasoning LLM agents</b> and <b>regret-based decision theory</b>.
</p>
<p>
<b><span style="color:magenta;"> I’m actively looking for postdoctoral opportunities in theoretical foundations of reinforcement learning or reasoning LLM research.</span></b>
</p>
</td>
</tr>
</table>
<table style="width:100%;margin:auto;">
<tr>
<td style="padding:2.5%;"><h2>Preprints</h2></td>
</tr>
</table>
<table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/reward_regret.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>A Regret Minimization Framework on Preference Learning in Large Language Models</h3>
<br>
Suhwan Kim*, <b>Taehyun Cho*</b>, Youngsoo Jang, Geonhyeong Kim, Yujin Kim, Moontae Lee, Jungwoo Lee<br>
<em>Submitted to ICML 2026</em><br>
<!--post content-->
<!--<p><p>RePO (Regret-based Preference Optimization) reframes RLHF as regret minimization rather than reward maximization. Motivated by the prospective and counterfactual nature of human feedback, RePO models preferences as behavior-conditioned assessments of relative suboptimality. It admits a closed-form update under KL-regularized RL and integrates seamlessly with direct preference optimization. Empirically, RePO achieves consistent gains on mathematical reasoning and human preference benchmarks, demonstrating a principled and effective alternative to reward-based RLHF.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/PLP3_example2.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>An Axiomatization of Process Score Model: Your Process-level Feedback is Not a Reward</h3>
<br>
<b>Taehyun Cho</b>, Suhwan Kim, Seungyub Han, Seokhun Ju, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee<br>
<em>Work In Progress</em><br>
<!--post content-->
<!--<p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/Async_2.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Off-policy Direct Preference Optimization with Monotonic Improvement Guarantee</h3>
<br>
Seungyub Han*, <b>Taehyun Cho*</b>, Seokhun Ju, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee<br>
<em>Work In Progress</em><br>
<!--post content-->
<!--<p>
</p>-->
</td>
</tr>
</table>
<p><br><br></p>
<table style="width:100%;margin:auto;">
<tr>
<td style="padding:2.5%;"><h2>Dissertation</h2></td>
</tr>
</table>
<table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/thesis.PNG" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>A Distributional Perspective on Human-Aligned Decision Making under Uncertainty</h3>
<br>
<b>Taehyun Cho*</b><br>
<em>Department of Electrical and Computer Engineering, Seoul National University</em><br>
<a href="/pdfs/thesis.pdf">paper</a> /
<!--post content-->
<!--<p>
</p>-->
</td>
</tr>
</table>
<p><br><br></p>
<table style="width:100%;margin:auto;">
<tr>
<td style="padding:2.5%;"><h2>International Conference</h2></td>
</tr>
</table>
<table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/PPL_img.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Policy-labeled Preference Learning: Is Preference Enough for RLHF?</h3>
<br>
<b>Taehyun Cho*</b>, Seokhun Ju*, Seungyub Han, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee<br>
<em>ICML 2025 <span style="color:red; font-weight:bold;">spotlight (Top 2.6%)</span></em><br>
<a href="https://arxiv.org/abs/2505.06273">paper</a> /
<a href="https://arxiv.org/abs/2505.06273">arxiv</a> /
<!--post content-->
<!--<p><p>Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward,
we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information.
We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/statistical_functional.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation</h3>
<br>
<b>Taehyun Cho</b>, Seungyub Han, Kyungjae Lee, Seokhun Ju, Dohyeong Kim, Jungwoo Lee<br>
<em>ICML 2025</em><br>
<a href="https://arxiv.org/pdf/2407.21260">paper</a> /
<a href="https://arxiv.org/abs/2407.21260">arxiv</a> /
<!--post content-->
<!--<p><p>Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly including nonlinear statistical functional. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/kim_spectral.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees</h3>
<br>
Dohyeong Kim, <b>Taehyun Cho</b>, Seungyub Han, Hojun Chung, Kyungjae Lee, Songhwai Oh<br>
<em>NeurIPS 2024</em><br>
<a href="https://arxiv.org/pdf/2405.18698">paper</a> /
<a href="https://arxiv.org/abs/2405.18698">arxiv</a> /
<!--post content-->
<!--<p><p>We propose a safe RL algorithm with spectral risk constraints, which shows convergence to an optimum in tabular settings.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/pqr_1.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion</h3>
<br>
<b>Taehyun Cho</b>, Seungyub Han, Heesoo Lee, Kyungjae Lee, Jungwoo Lee<br>
<em>NeurIPS 2023</em><br>
<a href="https://openreview.net/forum?id=v8u3EFAyW9">paper</a> /
<a href="https://arxiv.org/abs/2310.16546">arxiv</a> /
<!--post content-->
<!--<p><p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/spqr.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning</h3>
<br>
Dohyeok Lee, Seungyub Han, <b>Taehyun Cho</b>, Jungwoo Lee<br>
<em>NeurIPS 2023</em><br>
<a href="https://openreview.net/forum?id=q0sdoFIfNg">paper</a> /
<a href="https://arxiv.org/abs/2401.03137">arxiv</a> /
<a href="https://github.com/dohyeoklee/SPQR">code</a> /
<!--post content-->
<!--<p><p>By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/nccl_uai.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>On the Convergence of Continual Learning with Adaptive Methods</h3>
<br>
Seungyub Han, Yeongmo Kim, <b>Taehyun Cho</b>, Jungwoo Lee<br>
<em>UAI 2023</em><br>
<a href="https://proceedings.mlr.press/v216/han23a.html">paper</a> /
<a href="https://arxiv.org/abs/2404.05555">arxiv</a> /
<!--post content-->
<!--<p><p>In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/nccl_opt_ml.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Adaptive Methods for Nonconvex Continual Learning</h3>
<br>
Seungyub Han, Yeongmo Kim, <b>Taehyun Cho</b>, Jungwoo Lee<br>
<em>NeurIPS 2022 Optimization for Machine Learning Workshop</em><br>
<a href="https://opt-ml.org/oldopt/papers/2022/paper14.pdf">paper</a> /
<!--post content-->
<!--<p><p>We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/pqr_neurips2022_workshop.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Perturbed Quantile Regression for Distributional Reinforcement Learning</h3>
<br>
<b>Taehyun Cho</b>, Seungyub Han, Heesoo Lee, Kyungjae Lee, Jungwoo Lee<br>
<em>NeurIPS 2022 Deep RL Workshop</em><br>
<a href="https://openreview.net/forum?id=-WXCYvc5E-P">paper</a> /
<!--post content-->
<!--<p><p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure in action selection, and prove the convergence and optimality of the proposed method by using the weaker contraction property.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/chebyshev.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Chebyshev polynomial codes: Task entanglement-based coding for distributed matrix multiplication</h3>
<br>
Sangwoo Hong, Heecheol Yang, Youngseok Yoon, <b>Taehyun Cho</b>, Jungwoo Lee<br>
<em>ICML 2021</em><br>
<a href="https://proceedings.mlr.press/v139/hong21b/hong21b.pdf">paper</a> /
<a href="https://proceedings.mlr.press/v139/hong21b.html">arxiv</a> /
<!--post content-->
<!--<p><p>We propose Chebyshev polynomial codes, which can achieve order-wise improvement in encoding complexity at the master and communication load in distributed matrix multiplication using task entanglement.
Chebyshev polynomial codes can provide significant reduction in overall processing time in distributed computing for matrix multiplication, which is a key computational component in modern deep learning.</p>
</p>-->
</td>
</tr>
</table>
<p><br><br></p>
<table style="width:100%;margin:auto;">
<tr>
<td style="padding:2.5%;"><h2>International Journal</h2></td>
</tr>
</table>
<table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/shallow.PNG" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>Optimized shallow neural networks for sum-rate maximization in energy harvesting downlink multiuser NOMA systems</h3>
<br>
Haesung Kim, <b>Taehyun Cho</b>, Jungwoo Lee, Wongae Shin, H Vincent Poor<br>
<em>IEEE Journal on Selected Areas in Communications</em><br>
<a href="https://ieeexplore.ieee.org/abstract/document/9174757">paper</a> /
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9174757">arxiv</a> /
<!--post content-->
<!--<p><p>This article considers a power allocation problem in energy harvesting downlink non-orthogonal multiple access (NOMA) systems in which a transmitter sends desired messages to their respective receivers by using harvested energy.
To tackle this problem, we make use of a reinforcement learning approach based on a shallow neural network structure.</p>
</p>-->
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;min-width:120px;vertical-align:middle;">
<img src="/images/energy_harvesting.png" alt="project image" style="max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle;">
<h3>An Efficient Neural Network Architecture for Rate Maximization in Energy Harvesting Downlink Channels</h3>
<br>
Haesung Kim, <b>Taehyun Cho</b>, Jungwoo Lee, Wonjae Shin, H Vincent Poor<br>
<em>2020 IEEE International Symposium on Information Theory (ISIT)</em><br>
<a href="https://ieeexplore.ieee.org/abstract/document/9174136">paper</a> /
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9174136">arxiv</a> /
<!--post content-->
<!--<p><p>We prove that the optimal power allocation policy that maximizes the sum-rate is an increasing function for harvested energy, channel gains, and remaining battery, regardless of the number of users in the downlink channels.
We use this proof as a mathematical basis for the construction of a shallow neural network that can fully reflect the increasing property of the optimal policy.</p>
</p>-->
</td>
</tr>
</table>
<p><br><br><!-- Footer --></p>
<table style="width:100%;margin:auto;">
<tr>
<td style="padding:0;">
<p style="text-align:center;font-size:small;">
Design and source code from <a href="https://jonbarron.info">Jon Barron's website</a>
</p>
</td>
</tr>
</table>
</td></tr>
  </table>
</body>
</html>
