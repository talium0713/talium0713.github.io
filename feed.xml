<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://talium0713.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://talium0713.github.io/" rel="alternate" type="text/html" /><updated>2025-07-12T06:28:33+00:00</updated><id>https://talium0713.github.io/feed.xml</id><title type="html">Taehyun Cho</title><entry><title type="html">An Axiomatization of Process Score Model: Your Process-level Feedback is Not a Reward</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="An Axiomatization of Process Score Model: Your Process-level Feedback is Not a Reward" /><published>2025-06-17T20:21:59+00:00</published><updated>2025-06-17T20:21:59+00:00</updated><id>https://talium0713.github.io/prm</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[]]></content><author><name>Taehyun Cho</name></author><category term="Preprint" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/PLP3_example2.png" /><media:content medium="image" url="https://talium0713.github.io/images/PLP3_example2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Off-policy Direct Preference Optimization with Monotonic Improvement Guarantee</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Off-policy Direct Preference Optimization with Monotonic Improvement Guarantee" /><published>2025-06-17T19:22:59+00:00</published><updated>2025-06-17T19:22:59+00:00</updated><id>https://talium0713.github.io/off_policy_DPO</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[]]></content><author><name>Seungyub Han</name></author><category term="Preprint" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Policy Optimization with Process Regret Model</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Policy Optimization with Process Regret Model" /><published>2025-06-17T19:21:59+00:00</published><updated>2025-06-17T19:21:59+00:00</updated><id>https://talium0713.github.io/ppl_llm</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[]]></content><author><name>Taehyun Cho</name></author><category term="Preprint" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Policy-labeled Preference Learning: Is Preference Enough for RLHF?</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Policy-labeled Preference Learning: Is Preference Enough for RLHF?" /><published>2025-05-06T20:21:59+00:00</published><updated>2025-05-06T20:21:59+00:00</updated><id>https://talium0713.github.io/ppl</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward, 
we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information. 
We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making.</p>]]></content><author><name>Taehyun Cho</name></author><category term="International Conference" /><summary type="html"><![CDATA[Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward, we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information. We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/PPL_img.png" /><media:content medium="image" url="https://talium0713.github.io/images/PPL_img.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation" /><published>2025-05-06T20:20:59+00:00</published><updated>2025-05-06T20:20:59+00:00</updated><id>https://talium0713.github.io/tractable</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly including nonlinear statistical functional. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$.</p>]]></content><author><name>Taehyun Cho</name></author><category term="International Conference" /><summary type="html"><![CDATA[Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly including nonlinear statistical functional. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/statistical_functional.png" /><media:content medium="image" url="https://talium0713.github.io/images/statistical_functional.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees" /><published>2024-10-10T22:21:59+00:00</published><updated>2024-10-10T22:21:59+00:00</updated><id>https://talium0713.github.io/spectral</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>We propose a safe RL algorithm with spectral risk constraints, which shows convergence to an optimum in tabular settings.</p>]]></content><author><name>Dohyeong Kim</name></author><category term="International Conference" /><summary type="html"><![CDATA[We propose a safe RL algorithm with spectral risk constraints, which shows convergence to an optimum in tabular settings.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/kim_spectral.png" /><media:content medium="image" url="https://talium0713.github.io/images/kim_spectral.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion" /><published>2023-12-07T22:21:59+00:00</published><updated>2023-12-07T22:21:59+00:00</updated><id>https://talium0713.github.io/pqr</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property.</p>]]></content><author><name>Taehyun Cho</name></author><category term="International Conference" /><summary type="html"><![CDATA[We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/pqr_1.png" /><media:content medium="image" url="https://talium0713.github.io/images/pqr_1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning" /><published>2023-12-07T20:21:59+00:00</published><updated>2023-12-07T20:21:59+00:00</updated><id>https://talium0713.github.io/spqr</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning.</p>]]></content><author><name>Dohyeok Lee</name></author><category term="International Conference" /><summary type="html"><![CDATA[By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/spqr.png" /><media:content medium="image" url="https://talium0713.github.io/images/spqr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">On the Convergence of Continual Learning with Adaptive Methods</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="On the Convergence of Continual Learning with Adaptive Methods" /><published>2023-08-12T22:21:59+00:00</published><updated>2023-08-12T22:21:59+00:00</updated><id>https://talium0713.github.io/nccl_uai</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.</p>]]></content><author><name>Seungyub Han</name></author><category term="International Conference" /><summary type="html"><![CDATA[In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/nccl_uai.png" /><media:content medium="image" url="https://talium0713.github.io/images/nccl_uai.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Perturbed Quantile Regression for Distributional Reinforcement Learning</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Perturbed Quantile Regression for Distributional Reinforcement Learning" /><published>2022-12-03T22:21:59+00:00</published><updated>2022-12-03T22:21:59+00:00</updated><id>https://talium0713.github.io/pqrl_neurips_workshop</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure in action selection, and prove the convergence and optimality of the proposed method by using the weaker contraction property.</p>]]></content><author><name>Taehyun Cho</name></author><category term="International Conference" /><summary type="html"><![CDATA[We provide a perturbed distributional Bellman optimality operator by distorting the risk measure in action selection, and prove the convergence and optimality of the proposed method by using the weaker contraction property.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/pqr_neurips2022_workshop.png" /><media:content medium="image" url="https://talium0713.github.io/images/pqr_neurips2022_workshop.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>