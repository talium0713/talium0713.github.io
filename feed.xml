<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://talium0713.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://talium0713.github.io/" rel="alternate" type="text/html" /><updated>2026-02-09T10:39:13+00:00</updated><id>https://talium0713.github.io/feed.xml</id><title type="html">Taehyun Cho</title><entry><title type="html">A Distributional Perspective on Human-Aligned Decision Making under Uncertainty</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="A Distributional Perspective on Human-Aligned Decision Making under Uncertainty" /><published>2026-02-06T19:22:59+00:00</published><updated>2026-02-06T19:22:59+00:00</updated><id>https://talium0713.github.io/thesis</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[]]></content><author><name>Taehyun Cho</name></author><category term="Dissertation" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/thesis.PNG" /><media:content medium="image" url="https://talium0713.github.io/images/thesis.PNG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Regret Minimization Framework on Preference Learning in Large Language Models</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="A Regret Minimization Framework on Preference Learning in Large Language Models" /><published>2026-01-28T20:21:59+00:00</published><updated>2026-01-28T20:21:59+00:00</updated><id>https://talium0713.github.io/ppl_llm</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>RePO (Regret-based Preference Optimization) reframes RLHF as regret minimization rather than reward maximization. Motivated by the prospective and counterfactual nature of human feedback, RePO models preferences as behavior-conditioned assessments of relative suboptimality. It admits a closed-form update under KL-regularized RL and integrates seamlessly with direct preference optimization. Empirically, RePO achieves consistent gains on mathematical reasoning and human preference benchmarks, demonstrating a principled and effective alternative to reward-based RLHF.</p>]]></content><author><name>Taehyun Cho</name></author><category term="Preprint" /><summary type="html"><![CDATA[RePO (Regret-based Preference Optimization) reframes RLHF as regret minimization rather than reward maximization. Motivated by the prospective and counterfactual nature of human feedback, RePO models preferences as behavior-conditioned assessments of relative suboptimality. It admits a closed-form update under KL-regularized RL and integrates seamlessly with direct preference optimization. Empirically, RePO achieves consistent gains on mathematical reasoning and human preference benchmarks, demonstrating a principled and effective alternative to reward-based RLHF.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/reward_regret.png" /><media:content medium="image" url="https://talium0713.github.io/images/reward_regret.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">An Axiomatization of Process Score Model: Your Process-level Feedback is Not a Reward</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="An Axiomatization of Process Score Model: Your Process-level Feedback is Not a Reward" /><published>2025-06-17T20:21:59+00:00</published><updated>2025-06-17T20:21:59+00:00</updated><id>https://talium0713.github.io/prm</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[]]></content><author><name>Taehyun Cho</name></author><category term="Preprint" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/PLP3_example2.png" /><media:content medium="image" url="https://talium0713.github.io/images/PLP3_example2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Off-policy Direct Preference Optimization with Monotonic Improvement Guarantee</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Off-policy Direct Preference Optimization with Monotonic Improvement Guarantee" /><published>2025-06-17T19:22:59+00:00</published><updated>2025-06-17T19:22:59+00:00</updated><id>https://talium0713.github.io/off_policy_DPO</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[]]></content><author><name>Seungyub Han</name></author><category term="Preprint" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/Async_2.png" /><media:content medium="image" url="https://talium0713.github.io/images/Async_2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Policy-labeled Preference Learning: Is Preference Enough for RLHF?</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Policy-labeled Preference Learning: Is Preference Enough for RLHF?" /><published>2025-05-06T20:21:59+00:00</published><updated>2025-05-06T20:21:59+00:00</updated><id>https://talium0713.github.io/ppl</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward, 
we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information. 
We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making.</p>]]></content><author><name>Taehyun Cho</name></author><category term="International Conference" /><summary type="html"><![CDATA[Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward, we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information. We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/PPL_img.png" /><media:content medium="image" url="https://talium0713.github.io/images/PPL_img.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation" /><published>2025-05-06T20:20:59+00:00</published><updated>2025-05-06T20:20:59+00:00</updated><id>https://talium0713.github.io/tractable</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly including nonlinear statistical functional. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$.</p>]]></content><author><name>Taehyun Cho</name></author><category term="International Conference" /><summary type="html"><![CDATA[Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly including nonlinear statistical functional. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/statistical_functional.png" /><media:content medium="image" url="https://talium0713.github.io/images/statistical_functional.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees" /><published>2024-10-10T22:21:59+00:00</published><updated>2024-10-10T22:21:59+00:00</updated><id>https://talium0713.github.io/spectral</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>We propose a safe RL algorithm with spectral risk constraints, which shows convergence to an optimum in tabular settings.</p>]]></content><author><name>Dohyeong Kim</name></author><category term="International Conference" /><summary type="html"><![CDATA[We propose a safe RL algorithm with spectral risk constraints, which shows convergence to an optimum in tabular settings.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/kim_spectral.png" /><media:content medium="image" url="https://talium0713.github.io/images/kim_spectral.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion" /><published>2023-12-07T22:21:59+00:00</published><updated>2023-12-07T22:21:59+00:00</updated><id>https://talium0713.github.io/pqr</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property.</p>]]></content><author><name>Taehyun Cho</name></author><category term="International Conference" /><summary type="html"><![CDATA[We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/pqr_1.png" /><media:content medium="image" url="https://talium0713.github.io/images/pqr_1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning" /><published>2023-12-07T20:21:59+00:00</published><updated>2023-12-07T20:21:59+00:00</updated><id>https://talium0713.github.io/spqr</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning.</p>]]></content><author><name>Dohyeok Lee</name></author><category term="International Conference" /><summary type="html"><![CDATA[By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/spqr.png" /><media:content medium="image" url="https://talium0713.github.io/images/spqr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">On the Convergence of Continual Learning with Adaptive Methods</title><link href="https://talium0713.github.io/" rel="alternate" type="text/html" title="On the Convergence of Continual Learning with Adaptive Methods" /><published>2023-08-12T22:21:59+00:00</published><updated>2023-08-12T22:21:59+00:00</updated><id>https://talium0713.github.io/nccl_uai</id><content type="html" xml:base="https://talium0713.github.io/"><![CDATA[<p>In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.</p>]]></content><author><name>Seungyub Han</name></author><category term="International Conference" /><summary type="html"><![CDATA[In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://talium0713.github.io/images/nccl_uai.png" /><media:content medium="image" url="https://talium0713.github.io/images/nccl_uai.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>